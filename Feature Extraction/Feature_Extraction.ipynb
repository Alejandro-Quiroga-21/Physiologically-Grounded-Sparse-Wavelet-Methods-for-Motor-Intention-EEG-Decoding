{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af64e7cd",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "#### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69fbb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1018c89",
   "metadata": {},
   "source": [
    "#### PSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bbbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSD\n",
    "\n",
    "def PSD(signals, fs=500, nperseg=500, overlap=250):\n",
    "    trials, samples = signals.shape\n",
    "    psd_values = []\n",
    "    for trial in range(trials):\n",
    "        freqs, psd = signal.welch(signals[trial], fs, nperseg=nperseg, noverlap=overlap, window='hamming', scaling='density')\n",
    "        psd_values.append(psd[0:40])\n",
    "    \n",
    "    psd_values = np.array(psd_values)\n",
    "    return psd_values, freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d1da7",
   "metadata": {},
   "source": [
    "#### DDWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdbbbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDWT\n",
    "\n",
    "def Wavelet(signals, fs=500, nperseg=500, overlap=250):\n",
    "    trials, samples = signals.shape\n",
    "    wavelet_values = []\n",
    "    for trial in range(trials):\n",
    "        cA5,cD5,cD4,cD3,cD2,cD1 = pywt.wavedec(signals[trial],'bior6.8',mode='periodization',level=5)\n",
    "        wavelet_values.append(np.concatenate((cD5,cD4)))\n",
    "    \n",
    "    wavelet_values = np.array(wavelet_values)\n",
    "    return wavelet_valuesfilter(l_freq=8, h_freq=30, fir_design='firwin', phase='zero-double', verbose=False) #verbose = True to print filter features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6c7a5c",
   "metadata": {},
   "source": [
    "#### WPT+LDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a45605",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WP_LDB():\n",
    "    def __init__(self, signals_array, classes_array, wavelet='bior2.8', levels_decomposition=5):\n",
    "        self.signals_array = signals_array\n",
    "        self.classes_array = classes_array\n",
    "        self.wavelet = wavelet\n",
    "        self.levels_decomposition = levels_decomposition\n",
    "    \n",
    "    def WP_energy(self):\n",
    "\n",
    "        def wp_decomposition(signal):\n",
    "            wpt = pywt.WaveletPacket(data=signal, wavelet=self.wavelet, mode='periodization',maxlevel=self.levels_decomposition)\n",
    "            return wpt\n",
    "        \n",
    "        def wp_energy_decomposition(wpt):\n",
    "            energy_per_node = {}\n",
    "            node_names = [] \n",
    "            total_coefficients_energy = 0.0\n",
    "            \n",
    "            for level in range(self.levels_decomposition + 1):\n",
    "                nodes_at_level = wpt.get_level(level)\n",
    "                for node in nodes_at_level:\n",
    "                    energy = np.sum(node.data ** 2)\n",
    "                    total_coefficients_energy += energy\n",
    "            \n",
    "            for level in range(self.levels_decomposition + 1):\n",
    "                nodes_at_level = wpt.get_level(level)\n",
    "                for node in nodes_at_level:\n",
    "                    energy = np.sum(node.data ** 2)\n",
    "                    energy_per_node[node.path] = energy / total_coefficients_energy\n",
    "                    node_names.append(node.path)           \n",
    "            return energy_per_node, node_names\n",
    "\n",
    "        def Energy_nodes(data, nodes_names):\n",
    "            energy_per_level = defaultdict(list)  \n",
    "            for trial in data: \n",
    "                for node, value in trial.items():  \n",
    "                    energy_per_level[node].append(value) \n",
    "            return dict(energy_per_level) \n",
    "\n",
    "        WP_class_1 = []\n",
    "        WP_class_0 = []\n",
    "        all_node_names = []\n",
    "        for signal, cls in zip(self.signals_array,self.classes_array):\n",
    "            wp = wp_decomposition(signal)\n",
    "            energy_per_node, node_names = wp_energy_decomposition(wp)\n",
    "            for name in node_names:\n",
    "                if name not in all_node_names:\n",
    "                    all_node_names.append(name)\n",
    "            if cls == 1:           \n",
    "                WP_class_1.append(energy_per_node)\n",
    "            else:\n",
    "                WP_class_0.append(energy_per_node)\n",
    "\n",
    "        for trial in WP_class_0 + WP_class_1:\n",
    "            for node in all_node_names:\n",
    "                if node not in trial:\n",
    "                    trial[node] = 0.0\n",
    "            \n",
    "        Enode_class0 = {node: [trial[node] for trial in WP_class_0] for node in all_node_names}\n",
    "        Enode_class1 = {node: [trial[node] for trial in WP_class_1] for node in all_node_names}\n",
    "\n",
    "        return Enode_class0, Enode_class1, all_node_names\n",
    "    \n",
    "    def WP_discrimination(self, Enode_class0, Enode_class1, nodes_names, dis_method = 'KL'):\n",
    "\n",
    "        def Fisher_Criteria(Class_0, Class_1):\n",
    "            mu_c0 = np.mean(Class_0)\n",
    "            mu_c1 = np.mean(Class_1)\n",
    "            sigma_c0 = np.var(Class_0, ddof=1)  \n",
    "            sigma_c1 = np.var(Class_1, ddof=1)\n",
    "    \n",
    "            denominator = sigma_c0 + sigma_c1\n",
    "            if denominator == 0:\n",
    "                return 0.0  \n",
    "            FD = (mu_c0 - mu_c1)**2 / denominator\n",
    "            return FD\n",
    "            # Cuanto mayor sea FD mas separadas est√°n las clases\n",
    "        \n",
    "        def kullback_Leibler(Class_0, Class_1, bins=20, epsilon=1e-10):\n",
    "            min_val = min(np.min(Class_0), np.min(Class_1))\n",
    "            max_val = max(np.max(Class_0), np.max(Class_1))\n",
    "            \n",
    "            hist_c0, _ = np.histogram(Class_0, bins=bins, range=(min_val, max_val), density=True)\n",
    "            hist_c1, _ = np.histogram(Class_1, bins=bins, range=(min_val, max_val), density=True)\n",
    "            \n",
    "            hist_c0 += epsilon\n",
    "            hist_c1 += epsilon\n",
    "            \n",
    "            p = hist_c0 / np.sum(hist_c0)\n",
    "            q = hist_c1 / np.sum(hist_c1)\n",
    "            \n",
    "            kl_divergence = np.sum(p * np.log(p / q))\n",
    "            return kl_divergence\n",
    "        \n",
    "        def diferential_energy(Class_0, Class_1):\n",
    "            energy_A = np.mean(np.square(Class_0))\n",
    "            energy_B = np.mean(np.square(Class_1))\n",
    "            return np.abs(energy_A - energy_B)\n",
    "        \n",
    "        def compute_renyi_entropy(coeffs, alpha=2, bins=20, epsilon=1e-10):\n",
    "            hist, _ = np.histogram(coeffs, bins=bins, density=True)\n",
    "            hist += epsilon\n",
    "            hist /= np.sum(hist) \n",
    "            if alpha == 1:\n",
    "                return -np.sum(hist * np.log(hist))  \n",
    "            else:\n",
    "                return (1 / (1 - alpha)) * np.log(np.sum(hist ** alpha))\n",
    "            \n",
    "        def renyi_diff(Class_0, Class_1, alpha=2, bins=20):\n",
    "            H0 = compute_renyi_entropy(Class_0, alpha, bins)\n",
    "            H1 = compute_renyi_entropy(Class_1, alpha, bins)\n",
    "            return np.abs(H0 - H1)\n",
    "\n",
    "        \n",
    "        Discriminant = []\n",
    "        for node in nodes_names:\n",
    "            if dis_method == 'KL':\n",
    "                Discriminant.append(kullback_Leibler(Enode_class0[node],Enode_class1[node]))\n",
    "            elif dis_method == 'FC':\n",
    "                Discriminant.append(Fisher_Criteria(Enode_class0[node],Enode_class1[node]))\n",
    "            elif dis_method == 'DE':\n",
    "                Discriminant.append(diferential_energy(Enode_class0[node],Enode_class1[node]))\n",
    "            elif dis_method == 'RY':\n",
    "                Discriminant.append(renyi_diff(Enode_class0[node],Enode_class1[node]))\n",
    "        nodes_names[0] = 'root'\n",
    "        return nodes_names, Discriminant\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, score, name):\n",
    "        self.score = score  \n",
    "        self.name = name\n",
    "        self.base = None    \n",
    "        self.a = None    \n",
    "        self.d = None\n",
    "\n",
    "    @staticmethod\n",
    "    def build_binary_tree(scores, names):\n",
    "        if not scores or scores[0] is None:\n",
    "            return None\n",
    "\n",
    "        root = Node(scores[0], names[0])\n",
    "        queue = deque([root])\n",
    "        index = 1\n",
    "\n",
    "        while queue and index < len(scores):\n",
    "            current_node = queue.popleft()\n",
    "\n",
    "            # Left child\n",
    "            if index < len(scores) and scores[index] is not None:\n",
    "                current_node.a = Node(scores[index], names[index])\n",
    "                queue.append(current_node.a)\n",
    "            index += 1\n",
    "\n",
    "            # Right child\n",
    "            if index < len(scores) and scores[index] is not None:\n",
    "                current_node.d = Node(scores[index], names[index])\n",
    "                queue.append(current_node.d)\n",
    "            index += 1\n",
    "\n",
    "        return root\n",
    "    \n",
    "    def print_tree(root):\n",
    "        if not root:\n",
    "            return\n",
    "\n",
    "        queue = deque([(root, 0)])  \n",
    "        niveles = {} \n",
    "\n",
    "        while queue:\n",
    "            node, level = queue.popleft()\n",
    "            \n",
    "            if level not in niveles:\n",
    "                niveles[level] = []\n",
    "            niveles[level].append(f\"{node.name}({node.score})\")\n",
    "            \n",
    "            if node.a:\n",
    "                queue.append((node.a, level + 1))\n",
    "            if node.d:\n",
    "                queue.append((node.d, level + 1))\n",
    "\n",
    "        for level in sorted(niveles.keys()):\n",
    "            print(\"    \" * (len(niveles) - level), \"  \".join(niveles[level]))\n",
    "\n",
    "    \n",
    "    def prune(self, modified_nodes = None):\n",
    "        if self is None:\n",
    "            return None\n",
    "        \n",
    "        if modified_nodes is None:\n",
    "            modified_nodes = []  \n",
    "\n",
    "        if self.a:\n",
    "            self.a = self.a.prune()\n",
    "        if self.d:\n",
    "            self.d = self.d.prune()\n",
    "\n",
    "        suma_hijos = 0\n",
    "        if self.a:\n",
    "            suma_hijos += self.a.score\n",
    "        if self.d:\n",
    "            suma_hijos += self.d.score\n",
    "\n",
    "        if suma_hijos > self.score:\n",
    "            if self.a and self.d:\n",
    "                self.score = self.a.score + self.d.score\n",
    "                modified_nodes.append(self.a.name)\n",
    "                modified_nodes.append(self.d.name) \n",
    "                self.name = f'{self.a.name}-{self.d.name}' \n",
    "                self.a = None\n",
    "                self.d = None\n",
    "        else:\n",
    "            if self.a:\n",
    "                self.a = None\n",
    "            if self.d:\n",
    "                self.d = None\n",
    "        \n",
    "        self.base = self.name.split('-')\n",
    "        return self\n",
    "    \n",
    "    def discriminant_base(self, discriminant, nodes):\n",
    "        index = []\n",
    "        scores = []\n",
    "        for node in self.base:\n",
    "            index.append(nodes.index(node))\n",
    "        for i in index:\n",
    "            scores.append(discriminant[i])\n",
    "        combination = list(zip(scores, self.base))\n",
    "        combination.sort(key=lambda x: x[0], reverse=True)\n",
    "        scores, nodes = zip(*combination)\n",
    "        #print(scores)\n",
    "        #print(nodes)\n",
    "        return nodes, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e3eed8",
   "metadata": {},
   "source": [
    "#### DAS-OMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fc0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionary\n",
    "def dictionary_WPT(samples=1024, max_level=5, mother_wavelet='db4'):\n",
    "    import pywt\n",
    "    from sklearn.preprocessing import normalize\n",
    "    # Base signal\n",
    "    signal = np.zeros(samples)\n",
    "\n",
    "    # Base tree\n",
    "    wp_original = pywt.WaveletPacket(data=signal, wavelet=mother_wavelet, mode='periodization', maxlevel=max_level)\n",
    "\n",
    "    dictionary = []\n",
    "    dictionary_columns = []\n",
    "\n",
    "    for level in range(1, max_level + 1):\n",
    "        nodes = wp_original.get_level(level, order='natural')\n",
    "        for node in nodes:\n",
    "            dictionary_columns.append([node.path, len(node.data)])\n",
    "            for i in range(len(node.data)):\n",
    "                # New Tree\n",
    "                wp_temp = pywt.WaveletPacket(data=None, wavelet=mother_wavelet, mode='periodization', maxlevel=max_level)\n",
    "\n",
    "                # Zero Coefficients\n",
    "                for other_node in wp_original.get_level(level, order='natural'):\n",
    "                    wp_temp[other_node.path] = np.zeros_like(other_node.data)\n",
    "\n",
    "                # One coefficient\n",
    "                impulse = np.zeros_like(node.data)\n",
    "                impulse[i] = 1\n",
    "                wp_temp[node.path] = impulse\n",
    "\n",
    "                # Rebuild signal (atom)\n",
    "                signal_rec = wp_temp.reconstruct(update=False)\n",
    "\n",
    "                # Normalize L2\n",
    "                norm = np.linalg.norm(signal_rec)\n",
    "                if norm > 0:\n",
    "                    signal_rec /= norm\n",
    "\n",
    "                dictionary.append(signal_rec)\n",
    "\n",
    "    dictionary = np.array(dictionary)\n",
    "    dictionary = normalize(dictionary, axis=1) \n",
    "    return dictionary_columns, dictionary\n",
    "\n",
    "### OMP\n",
    "def omp(x, D, T):\n",
    "    n, K = D.shape\n",
    "    r = x.copy()\n",
    "    support = []\n",
    "    alpha = np.zeros(n)\n",
    "\n",
    "    for _ in range(T):\n",
    "        projections = D @ r  \n",
    "\n",
    "        scores = np.abs(projections)\n",
    "\n",
    "        scores[support] = -np.inf\n",
    "\n",
    "\n",
    "        j_star = np.argmax(scores)\n",
    "        support.append(j_star)\n",
    "\n",
    "\n",
    "        D_sub = D.T[:, support]\n",
    "        alpha_sub = np.linalg.pinv(D_sub) @ x\n",
    "\n",
    "\n",
    "        r = x - D_sub @ alpha_sub\n",
    "\n",
    "\n",
    "    alpha[support] = alpha_sub\n",
    "\n",
    "    return alpha, support\n",
    "\n",
    "### AFM\n",
    "def afm_function(D, support_c1, support_c2):\n",
    "    K = D.shape[0] \n",
    "    frq_c1 = np.zeros(K)\n",
    "    frq_c2 = np.zeros(K)\n",
    "\n",
    "    # Class 0\n",
    "    for support in support_c1:         \n",
    "        frq_c1[support] += 1\n",
    "    frq_c1 /= len(support_c1)          \n",
    "\n",
    "    # Class 1\n",
    "    for support in support_c2:\n",
    "        frq_c2[support] += 1\n",
    "    frq_c2 /= len(support_c2)\n",
    "\n",
    "\n",
    "    afm = np.zeros(K)\n",
    "\n",
    "    for j in range(K):\n",
    "        p1, p2 = frq_c1[j], frq_c2[j]\n",
    "        pj_plus = max(p1, p2)\n",
    "        pj_star = min(p1, p2)\n",
    "        if pj_plus > 0:\n",
    "            afm[j] = (pj_plus - pj_star) / pj_plus\n",
    "        else:\n",
    "            afm[j] = 0\n",
    "\n",
    "    return afm\n",
    "\n",
    "### MCM\n",
    "def mcm_fuction(alpha_c1, alpha_c2):\n",
    "    K = alpha_c1[0].shape[0]\n",
    "\n",
    "    avg_c1 = np.mean([np.abs(a) for a in alpha_c1], axis=0)\n",
    "    avg_c2 = np.mean([np.abs(a) for a in alpha_c2], axis=0)\n",
    "\n",
    "    mcm = np.zeros(K)\n",
    "    for j in range(K):\n",
    "        pj1, pj2 = avg_c1[j], avg_c2[j]\n",
    "        pj_plus = max(pj1, pj2)\n",
    "        pj_star = min(pj1, pj2)\n",
    "        if pj_plus > 0:\n",
    "            mcm[j] = (pj_plus - pj_star) / pj_plus\n",
    "        else:\n",
    "            mcm[j] = 0\n",
    "\n",
    "    return mcm\n",
    "\n",
    "### REM\n",
    "def group_by_class(X, y, num_classes):\n",
    "    X_classes = [[] for _ in range(num_classes)]\n",
    "    \n",
    "    for x, label in zip(X, y):\n",
    "        if label == 0 :    \n",
    "            X_classes[0].append(x)\n",
    "        else:\n",
    "            X_classes[1].append(x)\n",
    "\n",
    "    return X_classes\n",
    "\n",
    "def rem_fast(X_classes, coeffs, D, num_classes):\n",
    "    K, N = D.shape  \n",
    "    mre = np.zeros(K)\n",
    "    plus_class = np.zeros(K, dtype=int)\n",
    "\n",
    "    recon_classes = []\n",
    "    for c in range(num_classes):\n",
    "        if len(X_classes[c]) == 0:\n",
    "            recon_classes.append([])\n",
    "            continue\n",
    "\n",
    "        Xc = np.stack(X_classes[c]) \n",
    "        Ac = np.stack(coeffs[c])  \n",
    "\n",
    "        recon_c = Ac @ D\n",
    "        recon_classes.append(recon_c)\n",
    "\n",
    "    for j in range(K):\n",
    "        errors = []\n",
    "        for c in range(num_classes):\n",
    "            if len(X_classes[c]) == 0:\n",
    "                errors.append(0)\n",
    "                continue\n",
    "\n",
    "            Xc = np.stack(X_classes[c])   \n",
    "            Ac = np.stack(coeffs[c])       \n",
    "            recon_full = recon_classes[c]  \n",
    "\n",
    "            recon_j = recon_full - np.outer(Ac[:, j], D.T[:, j]) \n",
    "\n",
    "            err_full = np.sum((Xc - recon_full)**2, axis=1)  \n",
    "            err_drop = np.sum((Xc - recon_j)**2, axis=1)     \n",
    "\n",
    "            errors.append(np.mean(err_drop - err_full))  \n",
    "\n",
    "\n",
    "        c_plus = np.argmax(errors)\n",
    "        r_plus = errors[c_plus]\n",
    "        r_star = np.max([e for k, e in enumerate(errors) if k != c_plus])\n",
    "\n",
    "        mre[j] = (r_plus - r_star) / r_plus if r_plus > 0 else 0\n",
    "        plus_class[j] = c_plus\n",
    "\n",
    "    return mre, plus_class\n",
    "\n",
    "def discriminative_score_grid(M_af, M_cm, M_rem, step=0.1):\n",
    "    K = len(M_af)\n",
    "    best_scores = None\n",
    "    best_alpha, best_beta = None, None\n",
    "    best_eval = -np.inf   \n",
    "\n",
    "\n",
    "    for alpha in np.arange(0, 1+step, step):\n",
    "        for beta in np.arange(0, 1+step, step):\n",
    "            if alpha + beta <= 1: \n",
    "                scores = alpha*M_af + beta*M_rem + (1 - alpha - beta)*M_cm\n",
    "                \n",
    "                eval_metric = np.var(scores)  \n",
    "                \n",
    "                if eval_metric > best_eval:\n",
    "                    best_eval = eval_metric\n",
    "                    best_scores = scores\n",
    "                    best_alpha, best_beta = alpha, beta\n",
    "    \n",
    "    return best_alpha, best_beta, best_scores\n",
    "\n",
    "### Dictionay per class\n",
    "def sub_dictionary(plus_class, best_scores, n_atoms_per_class = 20, num_classes = 2):\n",
    "    subdicts = []\n",
    "    selected_indices_per_class = []\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        idx_class_c = np.where(plus_class == c)[0]             \n",
    "        idx_sorted = idx_class_c[np.argsort(-best_scores[idx_class_c])] \n",
    "        selected_idx = idx_sorted[:n_atoms_per_class]\n",
    "        subdicts.append(D.T[:, selected_idx])\n",
    "        selected_indices_per_class.append(selected_idx)\n",
    "\n",
    "    final_dict = np.hstack(subdicts)  \n",
    "\n",
    "    \n",
    "    return final_dict, atoms_index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
